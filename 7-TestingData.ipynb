{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data\n",
    "\n",
    "This notebook contains the implementation of the best models on the test data. In addition, there are models that are also applied to 2 models from other studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, pickle, time\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSMultistepSplit:\n",
    "    \"\"\"\n",
    "    This class performs data splitting for Walk-Forward\n",
    "    Validation with sliding window 1. The splitting data\n",
    "    will be used in the LSTM for multistep forecasting or\n",
    "    not (it's up to you depending on the `n_steps` parameter).\n",
    "    We can divide the data into 3 categories, training, early\n",
    "    stopping and testing. If you don't use early stopping,\n",
    "    set `with_early_stopping_set` to `false`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits, n_steps, look_back):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_splits (int): How many fold you will used?\n",
    "\n",
    "            n_steps (int): How many days do you predict?\n",
    "            If larger than 1 its means you will perform multi-step forecasting\n",
    "\n",
    "            look_back (int): The number of days it takes to make a prediction.\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.n_steps = n_steps\n",
    "        self.look_back = look_back\n",
    "\n",
    "    def split(self, X, with_early_stopping_set=True):\n",
    "        \"\"\"\n",
    "        This method returns index for training, early \n",
    "        stopping and test dataset. Below the example to\n",
    "        use this class and method\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "            tss = TSMultistepSplit()\n",
    "            splits = tss.split(x, with_early_stopping_set=True)\n",
    "            for train_indices, early_stopping_indices, test_indices in splits:\n",
    "                print(train_indices, early_stopping_indices, test_indices)\n",
    "                # your code        \n",
    "        ```\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        # The number of data will used for early stopping and test data\n",
    "        n_out = self.n_steps * 2 if with_early_stopping_set else self.n_steps\n",
    "    \n",
    "        n_train = n_samples - (n_out + self.n_splits) + 1\n",
    "        if n_train < self.n_steps + self.look_back + self.n_splits:\n",
    "            print(\"Sample size don't enough to make train data\")\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            end_train = n_train + i\n",
    "            train_set = indices[:end_train]\n",
    "\n",
    "            if with_early_stopping_set:\n",
    "                early_stopping_set = indices[\n",
    "                    end_train - self.look_back : \n",
    "                    end_train + self.n_steps\n",
    "                ]\n",
    "\n",
    "                test_set = indices[\n",
    "                    end_train + self.n_steps - self.look_back : \n",
    "                    end_train + self.n_steps + self.n_steps\n",
    "                ]\n",
    "\n",
    "                yield train_set, early_stopping_set, test_set\n",
    "            else:\n",
    "                test_set = indices[\n",
    "                    end_train - self.look_back : \n",
    "                    end_train + self.n_steps\n",
    "                ]\n",
    "                yield train_set, test_set\n",
    "\n",
    "\n",
    "class DataStore():\n",
    "    \"\"\"\n",
    "    This class is used for storing data. The format support \n",
    "    for RNN model, like LSTM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, data, target_column, \n",
    "        look_back, n_steps, format=None,\n",
    "        scaler_x=None, scaler_y=None, \n",
    "        default_scaler=MinMaxScaler\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            data: Data will be used. `data` must have a column named `target column`\n",
    "            \n",
    "            target_column: Name of dependent variabel\n",
    "            \n",
    "            look_back: The number of days it takes to make a prediction.\n",
    "            \n",
    "            n_steps: How many days do you predict? If larger than 1 its means you will \n",
    "            perform multi-step forecasting\n",
    "            \n",
    "            format (optional): Just support RNN format. Defaults to None.\n",
    "            \n",
    "            scaler_x (optional): Object for transform independent data. \n",
    "            That object must have `transform` method and have been fitted. \n",
    "            If empty then the scaler to be used is the scaler defined in the \n",
    "            `default_scaler` parameter. Defaults to None.\n",
    "            \n",
    "            scaler_y (optional): Object for transform dependent data. \n",
    "            That object must have `transform` method and have been fitted. \n",
    "            If empty then the scaler to be used is the scaler defined in the \n",
    "            `default_scaler` parameter. Defaults to None.\n",
    "            \n",
    "            default_scaler (optional): The scaler must have `fit` and `transform` \n",
    "            methods. Preferably use a scaler from Scikit Learn. Defaults to MinMaxScaler.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data.copy()\n",
    "        self.target_column = target_column\n",
    "        self.look_back = look_back\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = data.shape[1] - 1\n",
    "        \n",
    "        self.scaler_x = default_scaler() if scaler_x is None else scaler_x\n",
    "        self.scaler_y = default_scaler() if scaler_y is None else scaler_y\n",
    "        self.defined_scaler_x = scaler_x is not None\n",
    "        self.defined_scaler_y = scaler_y is not None\n",
    "        \n",
    "        if format == \"rnn\": self.format_for_rnn()\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        To save this object to file with pickle\n",
    "        \"\"\"\n",
    "        with open(path, 'wb') as outp:\n",
    "            pickle.dump(self, outp, pickle.HIGHEST_PROTOCOL)\n",
    "               \n",
    "    def format_for_rnn(self):\n",
    "        \"\"\"\n",
    "        Format data for used in RNN model that have 3 dimension, namely\n",
    "        (n_data, look_back, n_features)\n",
    "        \"\"\"\n",
    "        (x, scaled_x), (y, scaled_y) = self.__get_independent_dependent_data(self.data, self.target_column)\n",
    "        \n",
    "        self.x, self.y = self.__lstm_output_vector(x, y)\n",
    "        self.scaled_x, self.scaled_y = self.__lstm_output_vector(scaled_x, scaled_y)\n",
    "        \n",
    "    def __get_independent_dependent_data(self,\n",
    "                                       data : pd.DataFrame, \n",
    "                                       target_column : str) -> tuple:\n",
    "\n",
    "        x = np.array(data)\n",
    "        y = np.array(data[target_column]).reshape(-1, 1)\n",
    "        \n",
    "        if not self.defined_scaler_x: self.scaler_x = self.scaler_x.fit(x)\n",
    "        if not self.defined_scaler_y: self.scaler_y = self.scaler_y.fit(y)\n",
    "        \n",
    "        scaled_x = self.scaler_x.transform(x)\n",
    "        scaled_y = self.scaler_y.transform(y).squeeze()\n",
    "        \n",
    "        return (x, scaled_x), (y, scaled_y)\n",
    "    \n",
    "    def __lstm_output_vector(self, data_x: np.ndarray,  data_y: np.ndarray) -> tuple:\n",
    "        x = []\n",
    "        y = []\n",
    "        n_data = len(data_x)\n",
    "        \n",
    "        for index in range(n_data):\n",
    "            index_end = index + self.look_back\n",
    "            index_end_output = index_end + self.n_steps\n",
    "            \n",
    "            if index_end_output > n_data: break\n",
    "            \n",
    "            x.append(data_x[index:index_end, :])\n",
    "            y.append(data_y[index_end:index_end_output])\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "    \n",
    "    \n",
    "class SuccessiveEarlyStopping(Callback):\n",
    "    \"\"\"\n",
    "    This class implements the successive early stops described in the paper: \n",
    "    \n",
    "    L. Prechelt, “Early Stopping - But When?,” in Neural Networks: Tricks of \n",
    "    the Trade, vol. 1524, G. B. Orr and K.-R. Müller, Eds. Berlin, Heidelberg: \n",
    "    Springer Berlin Heidelberg, 1998, pp. 55–69. doi: 10.1007/3-540-49430-8_3.\n",
    "    \n",
    "    This class extends Callback class from Keras Tensorflow so it's easy to\n",
    "    integrate with keras model.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=0, monitor='val_loss', min_epochs=30):\n",
    "        \"\"\"\n",
    "        If as many as `patience` times the `monitor` goes up in a row, \n",
    "        the training process will be stopped\n",
    "        \"\"\"\n",
    "        \n",
    "        super(SuccessiveEarlyStopping, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.monitor = monitor\n",
    "        self.min_epochs = min_epochs\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"\n",
    "        This method will be executed when the modeling process starts. \n",
    "        This method will initialize some configuration\n",
    "        \"\"\"\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = np.Inf\n",
    "        self.last_epoch_loss = np.Inf\n",
    "        self.best_weights_epoch = 0\n",
    "        self.last_loss_down = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Every epoch end, this method will be executed to\n",
    "        check if the termination criteria are met.\n",
    "        \"\"\"\n",
    "        current = logs.get(self.monitor)\n",
    "        \n",
    "        if np.less(current, self.last_epoch_loss):\n",
    "            if np.less(current, self.best): \n",
    "                self.best = current\n",
    "                self.best_weights = self.model.get_weights()\n",
    "                self.best_weights_epoch = epoch + 1\n",
    "                \n",
    "            self.wait = 0\n",
    "            self.last_loss_down = epoch + 1\n",
    "        else:\n",
    "            if epoch + 1 >= self.min_epochs:\n",
    "                self.wait += 1\n",
    "                \n",
    "            if (self.wait >= self.patience):\n",
    "                self.model.stop_training = True\n",
    "                print(f\"Early Stopping on epoch {epoch + 1}\")\n",
    "                                \n",
    "        self.last_epoch_loss = current\n",
    "        self.stopped_epoch = epoch + 1\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"\n",
    "        This method will be executed when the modeling process end.\n",
    "        When the termination criteria are met. This method will \n",
    "        restore weight to best weights\n",
    "        \"\"\"\n",
    "        if np.less(self.best, self.last_epoch_loss):\n",
    "            print(f\"Restoring model weights from epoch {self.best_weights_epoch} with {self.monitor} = {self.best}\")\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            \n",
    "class WalkForwardValidation():\n",
    "    \"\"\"\n",
    "    This class implements Walk Forward validation\n",
    "    without hyperparameter optimization. There are \n",
    "    3 models implement in this step. First is the \n",
    "    best combination variabel with best hyperparameter.\n",
    "    Second and third from other publication\n",
    "        1.  A. Heiden and R. S. Parpinelli, “Applying LSTM for Stock \n",
    "            e Prediction with Sentiment Analysis,” in Anais do 15. Congresso \n",
    "            Brasileiro de Inteligência Computacional, Jan. 2021, \n",
    "            pp. 1–8. doi: 10.21528/CBIC2021-45.\n",
    "       \n",
    "        2.  S. T. Mndawe, B. S. Paul, and W. Doorsamy, “Development of \n",
    "            a Stock Price Prediction Framework for Intelligent Media \n",
    "            and Technical Analysis,” Appl. Sci., vol. 12, no. 2, \n",
    "            p. 719, Jan. 2022, doi: 10.3390/app12020719\n",
    "    \"\"\"\n",
    "    def __init__(self, target_column, n_splits, n_steps, max_epochs=100, verbose_fit_model=2):\n",
    "        self.target_column = target_column\n",
    "        self.n_splits = n_splits\n",
    "        self.n_steps = n_steps\n",
    "        self.max_epochs = max_epochs\n",
    "        self.verbose_fit_model = verbose_fit_model\n",
    "        \n",
    "    def get_metrics(self, errors):\n",
    "        squared_errors = np.power(errors, 2)\n",
    "        abs_errors = np.abs(errors)\n",
    "        return {\n",
    "            'rmse_total' : np.sqrt(squared_errors.mean(axis=1)).mean(),\n",
    "            'rmse_eachday' : np.sqrt(squared_errors.mean(axis=0)).tolist(),\n",
    "            'mae_total' : abs_errors.mean(axis=1).mean(),\n",
    "            'mae_eachday' : abs_errors.mean(axis=0).tolist()\n",
    "        }\n",
    "            \n",
    "    def _get_callbacks(self):\n",
    "        early_stopping = SuccessiveEarlyStopping(\n",
    "            patience=3, monitor='val_loss', min_epochs=40\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=20, min_lr=1e-7\n",
    "        )\n",
    "        \n",
    "        return early_stopping, reduce_lr\n",
    "    \n",
    "    def _data_store(\n",
    "        self, data, index, look_back,\n",
    "        scaler_x=None, scaler_y=None\n",
    "    ):\n",
    "        return DataStore(\n",
    "            data=data.iloc[index, :],\n",
    "            target_column=self.target_column,\n",
    "            look_back=look_back,\n",
    "            n_steps=self.n_steps,\n",
    "            scaler_x=scaler_x,\n",
    "            scaler_y=scaler_y,\n",
    "            format=\"rnn\"\n",
    "        )\n",
    "        \n",
    "    def __get_data(self, data, train_indices, test_indices, look_back, es_indices=None):\n",
    "        data_train = self._data_store(data, train_indices, look_back)\n",
    "        data_test = self._data_store(\n",
    "            data, test_indices, look_back,\n",
    "            scaler_x=data_train.scaler_x,\n",
    "            scaler_y=data_train.scaler_y\n",
    "        )\n",
    "        if es_indices is not None:\n",
    "            data_early_stopping = self._data_store(\n",
    "                data, es_indices, look_back, \n",
    "                scaler_x=data_train.scaler_x, \n",
    "                scaler_y=data_train.scaler_y\n",
    "            )\n",
    "            return data_train, data_early_stopping, data_test\n",
    "        \n",
    "        return data_train, data_test\n",
    "    \n",
    "    def get_prediction(self, model, x, scaler=None):\n",
    "        prediction = model.predict(x)\n",
    "        if scaler is not None:\n",
    "            prediction = np.array(prediction).reshape(-1, 1)\n",
    "            prediction = scaler.inverse_transform(prediction)\n",
    "        return prediction\n",
    "    \n",
    "    def convert2float(self, data: list):\n",
    "        return [float(x) for x in data]\n",
    "    \n",
    "    def _save_file2json(self, file, fname):\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write(json.dumps(file))\n",
    "            \n",
    "    def _get_model_default(self, hp, n_features):\n",
    "        look_back = hp['look_back']\n",
    "\n",
    "        model = Sequential()\n",
    "        num_layers = hp[\"num_layers\"]\n",
    "        is_first_layer = True\n",
    "        for i in range(num_layers):        \n",
    "            \n",
    "            if is_first_layer:\n",
    "                model.add(Input(shape=(look_back, n_features)))\n",
    "                is_first_layer = False\n",
    "                \n",
    "            is_last_layer = i == num_layers - 1\n",
    "            return_sequences = not is_last_layer    \n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    units=hp[f\"units_{i}\"],\n",
    "                    return_sequences=return_sequences, name=f\"lstm_layer_{i}\"\n",
    "                )\n",
    "            )\n",
    "            model.add(Dropout(\n",
    "                rate=hp[f'dropout_rate_{i}'],\n",
    "                name=f\"dropuout_layer_{i}\"\n",
    "            ))\n",
    "\n",
    "        model.add(Dense(self.n_steps, name=\"dense_layer_output\"))\n",
    "        \n",
    "        optimizer = Adam if hp[\"optimizer\"] == \"adam\" else RMSprop\n",
    "        learning_rate = hp[\"lr\"]\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer(learning_rate=learning_rate), loss=\"mse\"\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _model_default(self, x, params, train_every_n_day=1):\n",
    "        look_back = params['look_back']\n",
    "        batch_size = params['batch_size']\n",
    "        n_features = x.shape[1]\n",
    "                \n",
    "        tss = TSMultistepSplit(\n",
    "            n_splits=self.n_splits, n_steps=self.n_steps, look_back=look_back\n",
    "        )\n",
    "\n",
    "        errors = []\n",
    "        summary_iter = []\n",
    "        iteration = 1\n",
    "        \n",
    "        splits = tss.split(x, with_early_stopping_set=True)\n",
    "        for train_indices, early_stopping_indices, test_indices in splits:\n",
    "            print(\"Fold ke-\", iteration)\n",
    "            train_in_this_fold = (iteration - 1) % train_every_n_day == 0\n",
    "            \n",
    "            if train_in_this_fold:\n",
    "                print('Train in this fold with last 3 indices:', train_indices[-3:], test_indices[-3:])\n",
    "                data_train, data_early_stopping, data_test = self.__get_data(\n",
    "                    x, train_indices, test_indices, look_back, \n",
    "                    early_stopping_indices\n",
    "                )\n",
    "                \n",
    "                last_train_indices = train_indices\n",
    "                last_early_stopping_indices = early_stopping_indices\n",
    "                \n",
    "                early_stopping, reduce_lr = self._get_callbacks()\n",
    "                \n",
    "                model = self._get_model_default(params, n_features)\n",
    "            \n",
    "                start_time = time.time()\n",
    "                history = model.fit(\n",
    "                    data_train.scaled_x, data_train.scaled_y, \n",
    "                    shuffle=False, batch_size=batch_size,\n",
    "                    epochs=self.max_epochs, verbose=self.verbose_fit_model, \n",
    "                    validation_data=(data_early_stopping.scaled_x, data_early_stopping.scaled_y), \n",
    "                    callbacks=[early_stopping, reduce_lr]\n",
    "                )\n",
    "                training_time = time.time() - start_time\n",
    "                \n",
    "            else :\n",
    "                print('No train in this fold with last 3 indices:', last_train_indices[-3:], test_indices[-3:])\n",
    "                data_train, data_early_stopping, data_test = self.__get_data(\n",
    "                    x, last_train_indices, test_indices, look_back, \n",
    "                    last_early_stopping_indices\n",
    "                )\n",
    "            \n",
    "            prediction_test = self.get_prediction(model, data_test.scaled_x, data_test.scaler_y)\n",
    "            prediction_es = self.get_prediction(model, data_early_stopping.scaled_x, data_early_stopping.scaler_y)\n",
    "            error = (prediction_test - data_test.y).reshape(-1)\n",
    "            errors.append(error)\n",
    "            \n",
    "            summary_iter.append({\n",
    "                'iteration' : iteration,\n",
    "                'rmse' : np.sqrt(np.power(error, 2).mean()),\n",
    "                'mae' : np.abs(error).mean(),\n",
    "                'best_epoch' : early_stopping.best_weights_epoch,\n",
    "                'stopped_epoch' : early_stopping.stopped_epoch,\n",
    "                'training_time' : training_time,\n",
    "                'prediction_early_stopping' : {\n",
    "                    'y_true' : data_early_stopping.y.reshape(-1).tolist(),\n",
    "                    'y_pred' : prediction_es.reshape(-1).tolist()\n",
    "                },\n",
    "                'prediction_test' : {\n",
    "                    'y_true' : data_test.y.reshape(-1).tolist(),\n",
    "                    'y_pred' : prediction_test.reshape(-1).tolist(),\n",
    "                },\n",
    "                'history' : {\n",
    "                    'loss' : self.convert2float(history.history['loss']),\n",
    "                    'val_loss' : self.convert2float(history.history['val_loss']),\n",
    "                    'lr' : self.convert2float(history.history['lr'])\n",
    "                }\n",
    "            })\n",
    "            iteration += 1\n",
    "            \n",
    "        return errors, summary_iter\n",
    "    \n",
    "    def _get_model_heiden_mndawe(self, hp, n_features, model_type):\n",
    "        look_back = 60 if model_type == 'heiden' else 20\n",
    "        num_layers = 2 if model_type == 'heiden' else 1\n",
    "        units = [64, 32] if model_type == 'heiden' else [200]\n",
    "\n",
    "        model = Sequential()\n",
    "        is_first_layer = True\n",
    "        for i in range(num_layers):        \n",
    "            \n",
    "            if is_first_layer:\n",
    "                model.add(Input(shape=(look_back, n_features)))\n",
    "                is_first_layer = False\n",
    "                \n",
    "            is_last_layer = i == num_layers - 1\n",
    "            return_sequences = not is_last_layer    \n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    units=units[i],\n",
    "                    return_sequences=return_sequences, name=f\"lstm_layer_{i}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if model_type == 'heiden':\n",
    "            model.add(Dense(self.n_steps, name=\"dense_layer_output\", activation='relu'))\n",
    "        else:\n",
    "            model.add(Dense(self.n_steps, name=\"dense_layer_output\"))\n",
    "        \n",
    "        optimizer = Adam if hp[\"optimizer\"] == \"adam\" else RMSprop\n",
    "        learning_rate = hp[\"lr\"] if model_type == 'heiden' else 0.0001\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer(learning_rate=learning_rate), loss=\"mse\"\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _model_heiden_mndawe(self, x, params, model_type='heiden', train_every_n_day=1):\n",
    "        look_back = 60 if model_type == 'heiden' else 20\n",
    "        batch_size = params['batch_size']\n",
    "        n_features = x.shape[1]\n",
    "                \n",
    "        tss = TSMultistepSplit(\n",
    "            n_splits=self.n_splits, n_steps=self.n_steps, look_back=look_back\n",
    "        )\n",
    "\n",
    "        errors = []\n",
    "        summary_iter = []\n",
    "        iteration = 1\n",
    "        \n",
    "        splits = tss.split(x, with_early_stopping_set=False)\n",
    "        for train_indices, test_indices in splits:\n",
    "            print(\"Fold ke-\", iteration)\n",
    "            train_in_this_fold = (iteration - 1) % train_every_n_day == 0\n",
    "            \n",
    "            if train_in_this_fold:\n",
    "                print(f'Train in this fold with last 3 indices', train_indices[-3:], test_indices[-3:])\n",
    "                data_train, data_test = self.__get_data(\n",
    "                    x, train_indices, test_indices, look_back, \n",
    "                )\n",
    "                last_train_indices = train_indices\n",
    "                \n",
    "                model = self._get_model_heiden_mndawe(params, n_features, model_type=model_type)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                history = model.fit(\n",
    "                    data_train.scaled_x, data_train.scaled_y, \n",
    "                    shuffle=False, batch_size=batch_size,\n",
    "                    epochs=100, verbose=self.verbose_fit_model\n",
    "                )\n",
    "                training_time = time.time() - start_time\n",
    "                # print(model.summary())\n",
    "                # model.save('model.h5')\n",
    "                \n",
    "            else :\n",
    "                print('No train in this fold with last 3 indices:', last_train_indices[-3:], test_indices[-3:])\n",
    "                data_train, data_test = self.__get_data(\n",
    "                    x, last_train_indices, test_indices, look_back, \n",
    "                )\n",
    "                # print(model.summary())\n",
    "                \n",
    "            \n",
    "            prediction_test = self.get_prediction(model, data_test.scaled_x, data_test.scaler_y)\n",
    "            error = (prediction_test - data_test.y).reshape(-1)\n",
    "            errors.append(error)\n",
    "            \n",
    "            summary_iter.append({\n",
    "                'iteration' : iteration,\n",
    "                'rmse' : np.sqrt(np.power(error, 2).mean()),\n",
    "                'mae' : np.abs(error).mean(),\n",
    "                'best_epoch' : 100,\n",
    "                'stopped_epoch' : 100,\n",
    "                'training_time' : training_time,\n",
    "                'prediction_test' : {\n",
    "                    'y_true' : data_test.y.reshape(-1).tolist(),\n",
    "                    'y_pred' : prediction_test.reshape(-1).tolist(),\n",
    "                },\n",
    "                'history' : {\n",
    "                    'loss' : self.convert2float(history.history['loss']),\n",
    "                }\n",
    "            })\n",
    "            iteration += 1\n",
    "            \n",
    "        return errors, summary_iter\n",
    "    \n",
    "    def fit(\n",
    "        self, x, params, foldername, directory=\"kombinasi_variabel\", \n",
    "        model_type=\"default\", train_every_n_day=1\n",
    "    ):\n",
    "        \n",
    "        if model_type == 'default' :\n",
    "            errors, summary_iter = self._model_default(x, params, train_every_n_day=train_every_n_day)  \n",
    "        else :\n",
    "            errors, summary_iter = self._model_heiden_mndawe(x, params, model_type=model_type, train_every_n_day=train_every_n_day)\n",
    "            \n",
    "        errors = np.array(errors)\n",
    "        metrics = self.get_metrics(errors)\n",
    "        \n",
    "        metrics['errors'] = errors.tolist()\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        foldername = os.path.join(directory, foldername)\n",
    "        if not os.path.exists(foldername):\n",
    "            os.makedirs(foldername)\n",
    "        \n",
    "        self._save_file2json(metrics, os.path.join(foldername, \"metrics.json\"))\n",
    "        self._save_file2json(summary_iter, os.path.join(foldername, \"summary_iter.json\"))\n",
    "        \n",
    "class SummaryHPO():\n",
    "    \"\"\"\n",
    "    This class for make summary of Hyperparameter Optimization Process.\n",
    "    This class will read all `metrics.json` file that generated when\n",
    "    run Hyperparamter Optimization\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path=None, root=None, with_details=False):\n",
    "        \"\"\"If `file_path` is not None, will be read from file even `root` is not None\n",
    "        \"\"\"\n",
    "        if file_path is None and root is None:\n",
    "            raise Exception('File Path and root can\\'t be None at the same time')\n",
    "        \n",
    "        self.data = pd.read_csv(file_path) if file_path is not None else self._read_from_folder(root)\n",
    "        \n",
    "        if not with_details and 'details' in self.data.columns:\n",
    "            self.data.drop('details', axis=1, inplace=True)\n",
    "          \n",
    "    def _read_from_folder(self, root):\n",
    "        \n",
    "        trials = []\n",
    "        for dir1 in os.listdir(root):\n",
    "            for dir2 in os.listdir(os.path.join(root, dir1)):\n",
    "                for dir3 in os.listdir(os.path.join(root, dir1, dir2)):\n",
    "                    dir4 = os.path.join(root, dir1, dir2, dir3)\n",
    "                    \n",
    "                    if not os.path.isdir(dir4) or dir3 == 'trial_data':                \n",
    "                        continue\n",
    "                \n",
    "                    with open(os.path.join(dir4, 'metrics.json'), 'r') as f:\n",
    "                        metrics = json.load(f)\n",
    "                        \n",
    "                    with open(os.path.join(dir4, 'trial.json'), 'r') as f:\n",
    "                        trial_json = json.load(f)\n",
    "                        \n",
    "                    with open(os.path.join(dir4, 'summary_iter.json'), 'r') as f:\n",
    "                        summary = json.load(f)\n",
    "                        summary = pd.DataFrame(summary)\n",
    "                           \n",
    "                    trials.append({\n",
    "                        'variabel' : dir1,\n",
    "                        'iterasi' : dir2,\n",
    "                        'trial' : dir3,\n",
    "                        'rmse_total' : metrics['rmse_total'], \n",
    "                        'mae_total' : metrics['mae_total'], \n",
    "                        'mean_best_epoch' : summary.best_epoch.mean(),\n",
    "                        'mean_stopped_epoch' : summary.stopped_epoch.mean(),\n",
    "                        'mean_training_time' : summary.training_time.mean(),\n",
    "                        'rmse_eachday' : metrics['rmse_eachday'], \n",
    "                        'mae_eachday' : metrics['mae_eachday'],\n",
    "                        'hyperparameters' : trial_json['hyperparameters']['values'],\n",
    "                        'details' : summary.to_json(orient='records')\n",
    "                    })               \n",
    "        trials = pd.DataFrame(trials)     \n",
    "        trials['kombinasi_variabel'] = trials.variabel.apply(self._define_kombinasi_variabel)\n",
    "        return trials\n",
    "    \n",
    "    def _define_kombinasi_variabel(self, variabel):\n",
    "        if variabel == 'close': return 'KV 1'\n",
    "        if variabel == 'close_sentiment': return 'KV 2'\n",
    "        if variabel == 'close_kurs': return 'KV 3'\n",
    "        if variabel == 'close_technical': return 'KV 4'\n",
    "        if variabel == 'close_sentiment_kurs': return 'KV 5'\n",
    "        if variabel == 'close_technical_sentiment': return 'KV 6'\n",
    "        if variabel == 'close_technical_kurs': return 'KV 7'\n",
    "        if variabel == 'close_technical_sentiment_kurs': return 'KV 8'\n",
    "        \n",
    "    def __groupby(self, columns, target='rmse_total'):\n",
    "        idx_best =  self.data.groupby(columns)[target].idxmin()\n",
    "        return self.data.loc[idx_best].reset_index(drop=True).sort_values(columns)\n",
    "    \n",
    "    @property\n",
    "    def best_per_kv(self):\n",
    "        return self.__groupby(['kombinasi_variabel'])\n",
    "    \n",
    "    @property\n",
    "    def best_per_kv_iterasi(self):\n",
    "        return self.__groupby(['kombinasi_variabel', 'iterasi'])\n",
    "    \n",
    "    @property\n",
    "    def mean_rmse(self):\n",
    "        per_kv = self.best_per_kv_iterasi\n",
    "        return per_kv.groupby('kombinasi_variabel')['rmse_total'].agg([np.mean, np.std, np.min, np.max])\n",
    "    \n",
    "    @property\n",
    "    def mean_mae(self):\n",
    "        per_kv = self.best_per_kv_iterasi\n",
    "        return per_kv.groupby('kombinasi_variabel')['mae_total'].agg([np.mean, np.std, np.min, np.max])\n",
    "    \n",
    "    @property\n",
    "    def best_params(self):\n",
    "        params  = []\n",
    "        \n",
    "        for index, row in self.best_per_kv.iterrows():\n",
    "            param = eval(row['hyperparameters'])\n",
    "            temp = {\n",
    "                'kombinasi_variabel' : row['kombinasi_variabel'],\n",
    "                'look_back' : param['look_back'],\n",
    "                'optimizer' : param['optimizer'],\n",
    "                'learning_rate' : param['lr'],\n",
    "                'num_layers' : param['num_layers']\n",
    "            }\n",
    "            \n",
    "            for i in range(1, 4):\n",
    "                temp[f'layer_{i}_unit'] = param[f\"units_{i-1}\"] if i <= param['num_layers'] else None\n",
    "                temp[f'layer_{i}_dropout'] = param[f\"units_{i-1}\"] if i <= param['num_layers'] else None\n",
    "\n",
    "            params.append(temp)\n",
    "\n",
    "        return pd.DataFrame(params)\n",
    "\n",
    "class RunWalkForwardValidation():\n",
    "    \"\"\"\n",
    "    This class is interface to run WalkForwardValidation class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, data, n_splits, n_steps, n_iter=10,\n",
    "        verbose_fit_model=0, directory=\"kombinasi_variabel\"\n",
    "    ):\n",
    "        self.n_splits = n_splits\n",
    "        self.n_steps = n_steps\n",
    "        self.data = data\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose_fit_model = verbose_fit_model\n",
    "        self.directory = directory\n",
    "        \n",
    "        self.wfv = WalkForwardValidation(\n",
    "            target_column='close', n_splits=n_splits, \n",
    "            n_steps=n_steps, verbose_fit_model=verbose_fit_model\n",
    "        )\n",
    "        \n",
    "    def __get_data(self, kv):\n",
    "        if kv == 'KV 1': return self.data[['close']]\n",
    "        if kv == 'KV 2': return self.data[['close', 'sentiment_score']]\n",
    "        if kv == 'KV 3': return self.data[['close', 'kurs']]\n",
    "        if kv == 'KV 4': return self.data.drop(['sentiment_score', 'sentiment_category_score', 'kurs'], axis=1)\n",
    "        if kv == 'KV 5': return self.data[['close', 'sentiment_score', 'kurs']]\n",
    "        if kv == 'KV 6': return self.data.drop(['sentiment_category_score', 'kurs'], axis=1)\n",
    "        if kv == 'KV 7': return self.data.drop(['sentiment_score', 'sentiment_category_score'], axis=1)\n",
    "        if kv == 'KV 8': return self.data.drop(['sentiment_category_score'], axis=1)\n",
    "        \n",
    "    def fit(self, params, iterasi=None, model_type='default', train_every_n_day=1):\n",
    "        for index, param in params.iterrows():\n",
    "            kv = param['kombinasi_variabel']\n",
    "            param = eval(param['hyperparameters'])\n",
    "            \n",
    "            iterasi = iterasi if iterasi is not None else range(1, self.n_iter + 1)\n",
    "            for i in iterasi:\n",
    "                clear_output(wait=True)\n",
    "                subfolder = f\"{kv}\" if model_type == 'default' else f\"{model_type}\"\n",
    "                subfolder = f\"{subfolder}-{train_every_n_day}_days\"\n",
    "                \n",
    "                print(f\"{subfolder} - Iterasi ke-{i}\")\n",
    "                \n",
    "                foldername = os.path.join(self.directory, subfolder, f\"iterasi_{i}\")\n",
    "                if os.path.exists(foldername):\n",
    "                    continue\n",
    "                \n",
    "                data = self.__get_data(kv)\n",
    "                self.wfv.fit(\n",
    "                    x = data, params=param, \n",
    "                    foldername=os.path.join(subfolder, f\"iterasi_{i}\"), \n",
    "                    directory=self.directory,\n",
    "                    model_type=model_type, train_every_n_day=train_every_n_day\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model in Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = SummaryHPO('data/summary_hpo_nodetail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bri = pd.read_csv('data/testing_data.csv', index_col=0)\n",
    "\n",
    "# n split = 57 karena data testing dari Januari - Maret 2022\n",
    "# berjumlah 61 observasi, dikurangi 5 karena akan memprediksi 5 hari kedepan\n",
    "n_splits = 57\n",
    "n_steps = 5\n",
    "n_iterasi = 10\n",
    "\n",
    "run = RunWalkForwardValidation(\n",
    "    data=bri, n_splits=n_splits, n_steps=n_steps, n_iter=n_iterasi, \n",
    "    verbose_fit_model=0, directory=\"testing_data\"\n",
    ")\n",
    "\n",
    "params = summary.best_per_kv[['kombinasi_variabel', 'hyperparameters']]\n",
    "\n",
    "# Hanya menggunakan model KV2 karena model tersebut merupakan model\n",
    "# terbaik berdasarkan hasil yang diperoleh\n",
    "params = params[params.kombinasi_variabel == \"KV 2\"]\n",
    "\n",
    "for m_type in ['heiden']:\n",
    "    for n_day in [1]:\n",
    "        run.fit(params, iterasi=[7, 8], train_every_n_day=n_day, model_type=m_type)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6f7feddf18c8ecc9fa0d09c723cea7fa4b6247af6f300fc40fbf3468d2ad1be"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('skripsi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "This notebook will perform hyperparameter optimization for each combination of variables. Because there are many hyperparameters to run, this notebook is run in 3 different places, namely `Google Colab`, `Gradient Paperspace`, and `Personal Laptop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import os, json, pickle, time, math, zipfile\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSMultistepSplit:\n",
    "    \"\"\"\n",
    "    This class performs data splitting for Walk-Forward\n",
    "    Validation with sliding window 1. The splitting data\n",
    "    will be used in the LSTM for multistep forecasting or\n",
    "    not (it's up to you depending on the `n_steps` parameter).\n",
    "    We can divide the data into 3 categories, training, early\n",
    "    stopping and testing. If you don't use early stopping,\n",
    "    set `with_early_stopping_set` to `false`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits, n_steps, look_back):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_splits (int): How many fold you will used?\n",
    "\n",
    "            n_steps (int): How many days do you predict?\n",
    "            If larger than 1 its means you will perform multi-step forecasting\n",
    "\n",
    "            look_back (int): The number of days it takes to make a prediction.\n",
    "        \"\"\"\n",
    "        self.n_splits = n_splits\n",
    "        self.n_steps = n_steps\n",
    "        self.look_back = look_back\n",
    "\n",
    "    def split(self, X, with_early_stopping_set=True):\n",
    "        \"\"\"\n",
    "        This method returns index for training, early \n",
    "        stopping and test dataset. Below the example to\n",
    "        use this class and method\n",
    "        \n",
    "        ```python\n",
    "        \n",
    "            tss = TSMultistepSplit()\n",
    "            splits = tss.split(x, with_early_stopping_set=True)\n",
    "            for train_indices, early_stopping_indices, test_indices in splits:\n",
    "                print(train_indices, early_stopping_indices, test_indices)\n",
    "                # your code        \n",
    "        ```\n",
    "        \"\"\"\n",
    "        \n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        # The number of data will used for early stopping and test data\n",
    "        n_out = self.n_steps * 2 if with_early_stopping_set else self.n_steps\n",
    "    \n",
    "        n_train = n_samples - (n_out + self.n_splits) + 1\n",
    "        if n_train < self.n_steps + self.look_back + self.n_splits:\n",
    "            print(\"Sample size don't enough to make train data\")\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            end_train = n_train + i\n",
    "            train_set = indices[:end_train]\n",
    "\n",
    "            if with_early_stopping_set:\n",
    "                early_stopping_set = indices[\n",
    "                    end_train - self.look_back : \n",
    "                    end_train + self.n_steps\n",
    "                ]\n",
    "\n",
    "                test_set = indices[\n",
    "                    end_train + self.n_steps - self.look_back : \n",
    "                    end_train + self.n_steps + self.n_steps\n",
    "                ]\n",
    "\n",
    "                yield train_set, early_stopping_set, test_set\n",
    "            else:\n",
    "                test_set = indices[\n",
    "                    end_train - self.look_back : \n",
    "                    end_train + self.n_steps\n",
    "                ]\n",
    "                yield train_set, test_set\n",
    "\n",
    "\n",
    "class DataStore():\n",
    "    \"\"\"\n",
    "    This class is used for storing data. The format support \n",
    "    for RNN model, like LSTM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, data, target_column, \n",
    "        look_back, n_steps, format=None,\n",
    "        scaler_x=None, scaler_y=None, \n",
    "        default_scaler=MinMaxScaler\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            data: Data will be used. `data` must have a column named `target column`\n",
    "            \n",
    "            target_column: Name of dependent variabel\n",
    "            \n",
    "            look_back: The number of days it takes to make a prediction.\n",
    "            \n",
    "            n_steps: How many days do you predict? If larger than 1 its means you will \n",
    "            perform multi-step forecasting\n",
    "            \n",
    "            format (optional): Just support RNN format. Defaults to None.\n",
    "            \n",
    "            scaler_x (optional): Object for transform independent data. \n",
    "            That object must have `transform` method and have been fitted. \n",
    "            If empty then the scaler to be used is the scaler defined in the \n",
    "            `default_scaler` parameter. Defaults to None.\n",
    "            \n",
    "            scaler_y (optional): Object for transform dependent data. \n",
    "            That object must have `transform` method and have been fitted. \n",
    "            If empty then the scaler to be used is the scaler defined in the \n",
    "            `default_scaler` parameter. Defaults to None.\n",
    "            \n",
    "            default_scaler (optional): The scaler must have `fit` and `transform` \n",
    "            methods. Preferably use a scaler from Scikit Learn. Defaults to MinMaxScaler.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data.copy()\n",
    "        self.target_column = target_column\n",
    "        self.look_back = look_back\n",
    "        self.n_steps = n_steps\n",
    "        self.n_features = data.shape[1] - 1\n",
    "        \n",
    "        self.scaler_x = default_scaler() if scaler_x is None else scaler_x\n",
    "        self.scaler_y = default_scaler() if scaler_y is None else scaler_y\n",
    "        self.defined_scaler_x = scaler_x is not None\n",
    "        self.defined_scaler_y = scaler_y is not None\n",
    "        \n",
    "        if format == \"rnn\": self.format_for_rnn()\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        To save this object to file with pickle\n",
    "        \"\"\"\n",
    "        with open(path, 'wb') as outp:\n",
    "            pickle.dump(self, outp, pickle.HIGHEST_PROTOCOL)\n",
    "               \n",
    "    def format_for_rnn(self):\n",
    "        \"\"\"\n",
    "        Format data for used in RNN model that have 3 dimension, namely\n",
    "        (n_data, look_back, n_features)\n",
    "        \"\"\"\n",
    "        (x, scaled_x), (y, scaled_y) = self.__get_independent_dependent_data(self.data, self.target_column)\n",
    "        \n",
    "        self.x, self.y = self.__lstm_output_vector(x, y)\n",
    "        self.scaled_x, self.scaled_y = self.__lstm_output_vector(scaled_x, scaled_y)\n",
    "        \n",
    "    def __get_independent_dependent_data(self,\n",
    "                                       data : pd.DataFrame, \n",
    "                                       target_column : str) -> tuple:\n",
    "\n",
    "        x = np.array(data)\n",
    "        y = np.array(data[target_column]).reshape(-1, 1)\n",
    "        \n",
    "        if not self.defined_scaler_x: self.scaler_x = self.scaler_x.fit(x)\n",
    "        if not self.defined_scaler_y: self.scaler_y = self.scaler_y.fit(y)\n",
    "        \n",
    "        scaled_x = self.scaler_x.transform(x)\n",
    "        scaled_y = self.scaler_y.transform(y).squeeze()\n",
    "        \n",
    "        return (x, scaled_x), (y, scaled_y)\n",
    "    \n",
    "    def __lstm_output_vector(self, data_x: np.ndarray,  data_y: np.ndarray) -> tuple:\n",
    "        x = []\n",
    "        y = []\n",
    "        n_data = len(data_x)\n",
    "        \n",
    "        for index in range(n_data):\n",
    "            index_end = index + self.look_back\n",
    "            index_end_output = index_end + self.n_steps\n",
    "            \n",
    "            if index_end_output > n_data: break\n",
    "            \n",
    "            x.append(data_x[index:index_end, :])\n",
    "            y.append(data_y[index_end:index_end_output])\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "    \n",
    "    \n",
    "class SuccessiveEarlyStopping(Callback):\n",
    "    \"\"\"\n",
    "    This class implements the successive early stops described in the paper: \n",
    "    \n",
    "    L. Prechelt, “Early Stopping - But When?,” in Neural Networks: Tricks of \n",
    "    the Trade, vol. 1524, G. B. Orr and K.-R. Müller, Eds. Berlin, Heidelberg: \n",
    "    Springer Berlin Heidelberg, 1998, pp. 55–69. doi: 10.1007/3-540-49430-8_3.\n",
    "    \n",
    "    This class extends Callback class from Keras Tensorflow so it's easy to\n",
    "    integrate with keras model.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=0, monitor='val_loss', min_epochs=30):\n",
    "        \"\"\"\n",
    "        If as many as `patience` times the `monitor` goes up in a row, \n",
    "        the training process will be stopped\n",
    "        \"\"\"\n",
    "        \n",
    "        super(SuccessiveEarlyStopping, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.monitor = monitor\n",
    "        self.min_epochs = min_epochs\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"\n",
    "        This method will be executed when the modeling process starts. \n",
    "        This method will initialize some configuration\n",
    "        \"\"\"\n",
    "        self.wait = 0\n",
    "        self.stopped_epoch = 0\n",
    "        self.best = np.Inf\n",
    "        self.last_epoch_loss = np.Inf\n",
    "        self.best_weights_epoch = 0\n",
    "        self.last_loss_down = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Every epoch end, this method will be executed to\n",
    "        check if the termination criteria are met.\n",
    "        \"\"\"\n",
    "        current = logs.get(self.monitor)\n",
    "        \n",
    "        if np.less(current, self.last_epoch_loss):\n",
    "            if np.less(current, self.best): \n",
    "                self.best = current\n",
    "                self.best_weights = self.model.get_weights()\n",
    "                self.best_weights_epoch = epoch + 1\n",
    "                \n",
    "            self.wait = 0\n",
    "            self.last_loss_down = epoch + 1\n",
    "        else:\n",
    "            if epoch + 1 >= self.min_epochs:\n",
    "                self.wait += 1\n",
    "                \n",
    "            if (self.wait >= self.patience):\n",
    "                self.model.stop_training = True\n",
    "                print(f\"Early Stopping on epoch {epoch + 1}\")\n",
    "                                \n",
    "        self.last_epoch_loss = current\n",
    "        self.stopped_epoch = epoch + 1\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"\n",
    "        This method will be executed when the modeling process end.\n",
    "        When the termination criteria are met. This method will \n",
    "        restore weight to best weights\n",
    "        \"\"\"\n",
    "        if np.less(self.best, self.last_epoch_loss):\n",
    "            print(f\"Restoring model weights from epoch {self.best_weights_epoch} with {self.monitor} = {self.best}\")\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            \n",
    "\n",
    "class MyTuner(kt.Tuner):\n",
    "    \"\"\"\n",
    "    This class extends Tuner class from Keras Tuner library.\n",
    "    The purpose of extending a class is to add some required methods, \n",
    "    like `get callback` and `data store`.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def __get_folder(self, trial_id, fname):\n",
    "        return os.path.join(self.get_trial_dir(trial_id), fname)\n",
    "        \n",
    "    def save_model(self, trial_id, model, step=0):\n",
    "        fname = self.__get_folder(trial_id, \"model.h5\")\n",
    "        model.save(fname)\n",
    "        \n",
    "    def _save_data(self, data_store, type_data, look_back, step=0):\n",
    "        fname = self.__get_folder('data', f\"{type_data}_{look_back}.pkl\")\n",
    "        data_store.save(fname)\n",
    "        \n",
    "    def _save_file2json(self, trial_id, file, fname):\n",
    "        fname = self.__get_folder(trial_id, fname)\n",
    "        with open(fname, 'w') as f:\n",
    "            f.write(json.dumps(file))\n",
    "            \n",
    "    def __load_model(self, trial_id):\n",
    "        fname = self.__get_folder((trial_id), \"model.h5\")\n",
    "        return tf.keras.models.load_model(fname)\n",
    "\n",
    "    def load_model(self, trial):\n",
    "        return self.__load_model(trial.trial_id)\n",
    "    \n",
    "    def _get_callbacks(self):\n",
    "        early_stopping = SuccessiveEarlyStopping(\n",
    "            patience=3, monitor='val_loss', min_epochs=40\n",
    "        )\n",
    "        \n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=20, min_lr=1e-7\n",
    "        )\n",
    "        \n",
    "        return early_stopping, reduce_lr\n",
    "    \n",
    "    def _data_store(\n",
    "        self, data, index, look_back,\n",
    "        scaler_x=None, scaler_y=None\n",
    "    ):\n",
    "        return DataStore(\n",
    "            data=data.iloc[index, :],\n",
    "            target_column=self.target_column,\n",
    "            look_back=look_back,\n",
    "            n_steps=self.n_steps,\n",
    "            scaler_x=scaler_x,\n",
    "            scaler_y=scaler_y,\n",
    "            format=\"rnn\"\n",
    "        )\n",
    "                \n",
    "    def get_metrics(self, errors):\n",
    "        squared_errors = np.power(errors, 2)\n",
    "        abs_errors = np.abs(errors)\n",
    "        return {\n",
    "            'rmse_total' : np.sqrt(squared_errors.mean(axis=1)).mean(),\n",
    "            'rmse_eachday' : np.sqrt(squared_errors.mean(axis=0)).tolist(),\n",
    "            'mae_total' : abs_errors.mean(axis=1).mean(),\n",
    "            'mae_eachday' : abs_errors.mean(axis=0).tolist()\n",
    "        }\n",
    "        \n",
    "    def load_filejson(self, trial_id, fname):\n",
    "        path_file = self.__get_folder(trial_id, fname) \n",
    "        with open(path_file, 'r') as file:\n",
    "            json_file = json.load(file)\n",
    "        return json_file\n",
    "    \n",
    "    def load_data(self, type, look_back):\n",
    "        path_data = self.__get_folder('data', f\"{type}_{look_back}.pkl\")\n",
    "        with open(path_data, 'rb') as pickle_file:\n",
    "            data = pickle.load(pickle_file)\n",
    "        return data\n",
    "        \n",
    "    def get_file_in_directory(\n",
    "        self, trial_id=None\n",
    "    ):\n",
    "        if trial_id is None:\n",
    "            trial_id = self.oracle.get_best_trials()[0].trial_id\n",
    "            \n",
    "        model = self.__load_model(trial_id)\n",
    "        trial = self.load_filejson(trial_id, \"trial.json\")\n",
    "        look_back = trial['hyperparameters']['values']['look_back']\n",
    "        \n",
    "        data_train = self.load_data(\"train\", look_back)\n",
    "        data_test = self.load_data(\"test\", look_back) \n",
    "        data_es = self.load_data(\"es\", look_back)           \n",
    "        metrics = self.load_filejson(trial_id, \"metrics.json\")\n",
    "        summary = self.load_filejson(trial_id, \"summary_iter.json\")\n",
    "        \n",
    "        return model, metrics, trial, summary, (data_train, data_es, data_test)\n",
    "    \n",
    "    \n",
    "class WFVTuner(MyTuner):\n",
    "    \"\"\"\n",
    "    This class extends MyTuner class. This class will implements \n",
    "    Walk Forward Validation with Keras Tuner. The main method \n",
    "    of this class is `run_trial` to start doing hyperparameter\n",
    "    optimization\n",
    "    \"\"\"\n",
    "    def __init__(self, target_column, n_splits, n_steps, max_epochs=100, verbose_fit_model=2, **kwargs):\n",
    "        self.target_column = target_column\n",
    "        self.n_splits = n_splits\n",
    "        self.n_steps = n_steps\n",
    "        self.max_epochs = max_epochs\n",
    "        self.verbose_fit_model = verbose_fit_model\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def __get_data(self, data, train_indices, test_indices, look_back, es_indices=None):\n",
    "        data_train = self._data_store(data, train_indices, look_back)\n",
    "        data_test = self._data_store(\n",
    "            data, test_indices, look_back,\n",
    "            scaler_x=data_train.scaler_x,\n",
    "            scaler_y=data_train.scaler_y\n",
    "        )\n",
    "        if es_indices is not None:\n",
    "            data_early_stopping = self._data_store(\n",
    "                data, es_indices, look_back, \n",
    "                scaler_x=data_train.scaler_x, \n",
    "                scaler_y=data_train.scaler_y\n",
    "            )\n",
    "            return data_train, data_early_stopping, data_test\n",
    "        \n",
    "        return data_train, data_test\n",
    "    \n",
    "    def get_prediction(self, model, x, scaler=None):\n",
    "        prediction = model.predict(x)\n",
    "        if scaler is not None:\n",
    "            prediction = np.array(prediction).reshape(-1, 1)\n",
    "            prediction = scaler.inverse_transform(prediction)\n",
    "        return prediction\n",
    "    \n",
    "    def convert2float(self, data: list):\n",
    "        return [float(x) for x in data]\n",
    "    \n",
    "    def run_trial(self, trial, x, y, *args, **kwargs):\n",
    "        look_back = trial.hyperparameters.get('look_back')    \n",
    "        batch_size = trial.hyperparameters.get('batch_size')\n",
    "        \n",
    "        # Split data        \n",
    "        tss = TSMultistepSplit(\n",
    "            n_splits=self.n_splits, n_steps=self.n_steps, look_back=look_back\n",
    "        )\n",
    "\n",
    "        errors = []\n",
    "        summary_iter = []\n",
    "        iteration = 1\n",
    "        \n",
    "        splits = tss.split(x, with_early_stopping_set=True)\n",
    "        for train_indices, early_stopping_indices, test_indices in splits:\n",
    "            print(\"Iterasi ke-\", iteration)\n",
    "            \n",
    "            data_train, data_early_stopping, data_test = self.__get_data(\n",
    "                x, train_indices, test_indices, look_back, \n",
    "                early_stopping_indices\n",
    "            )\n",
    "            \n",
    "            early_stopping, reduce_lr = self._get_callbacks()\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            history = model.fit(\n",
    "                data_train.scaled_x, data_train.scaled_y, \n",
    "                shuffle=False, batch_size=batch_size,\n",
    "                epochs=self.max_epochs, verbose=self.verbose_fit_model, \n",
    "                validation_data=(data_early_stopping.scaled_x, data_early_stopping.scaled_y), \n",
    "                callbacks=[early_stopping, reduce_lr]\n",
    "            )\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Predist early stopiing and prediction data\n",
    "            prediction_es = self.get_prediction(model, data_early_stopping.scaled_x, data_early_stopping.scaler_y)\n",
    "            prediction_test = self.get_prediction(model, data_test.scaled_x, data_test.scaler_y)\n",
    "            error = (prediction_test - data_test.y).reshape(-1)\n",
    "            errors.append(error)\n",
    "            \n",
    "            summary_iter.append({\n",
    "                'iteration' : iteration,\n",
    "                'rmse' : np.sqrt(np.power(error, 2).mean()),\n",
    "                'mae' : np.abs(error).mean(),\n",
    "                'best_epoch' : early_stopping.best_weights_epoch,\n",
    "                'stopped_epoch' : early_stopping.stopped_epoch,\n",
    "                'training_time' : training_time,\n",
    "                'prediction_early_stopping' : {\n",
    "                    'y_true' : data_early_stopping.y.reshape(-1).tolist(),\n",
    "                    'y_pred' : prediction_es.reshape(-1).tolist()\n",
    "                },\n",
    "                'prediction_test' : {\n",
    "                    'y_true' : data_test.y.reshape(-1).tolist(),\n",
    "                    'y_pred' : prediction_test.reshape(-1).tolist(),\n",
    "                },\n",
    "                'history' : {\n",
    "                    'loss' : self.convert2float(history.history['loss']),\n",
    "                    'val_loss' : self.convert2float(history.history['val_loss']),\n",
    "                    'lr' : self.convert2float(history.history['lr'])\n",
    "                }\n",
    "            })\n",
    "            iteration += 1\n",
    "            \n",
    "        errors = np.array(errors)\n",
    "        metrics = self.get_metrics(errors)\n",
    "        \n",
    "        trial_id = trial.trial_id\n",
    "        self.oracle.update_trial(trial_id, {\n",
    "            'val_rmse': metrics['rmse_total'],\n",
    "            'val_mae': metrics['mae_total']\n",
    "        })\n",
    "        \n",
    "        # Save model and metrics\n",
    "        model._name = f\"model_{trial_id}\"\n",
    "        self.save_model(trial_id, model)\n",
    "        self._save_data(data_train, \"train\", look_back)\n",
    "        self._save_data(data_early_stopping, 'es', look_back)\n",
    "        self._save_data(data_test, \"test\", look_back)\n",
    "        \n",
    "        metrics['errors'] = errors.tolist()\n",
    "        self._save_file2json(trial_id, metrics, \"metrics.json\")\n",
    "        self._save_file2json(trial_id, summary_iter, \"summary_iter.json\")\n",
    "        \n",
    "\n",
    "class OneVectorHyperModel(kt.HyperModel):\n",
    "    \"\"\"\n",
    "    This class implements HyperModel for this research\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_steps, **kwargs):\n",
    "        self.n_features = n_features\n",
    "        self.n_steps = n_steps\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, hp):\n",
    "        \n",
    "        look_back = hp.Choice('look_back', [5, 10, 20, 30])\n",
    "        batch_size = hp.Int('batch_size', 0, 64, step=16)\n",
    "    \n",
    "        model = Sequential()\n",
    "        num_layers = hp.Int(\"num_layers\", min_value=1, max_value=3)\n",
    "        is_first_layer = True\n",
    "        for i in range(num_layers):        \n",
    "            \n",
    "            if is_first_layer:\n",
    "                model.add(Input(shape=(look_back, self.n_features)))\n",
    "                is_first_layer = False\n",
    "                \n",
    "            is_last_layer = i == num_layers - 1\n",
    "            return_sequences = not is_last_layer    \n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    units=hp.Int(f\"units_{i}\", min_value=100, max_value=500, step=50),\n",
    "                    return_sequences=return_sequences, name=f\"lstm_layer_{i}\"\n",
    "                )\n",
    "            )\n",
    "            model.add(Dropout(\n",
    "                rate=hp.Float(f'dropout_rate_{i}', min_value=0, max_value=0.5), \n",
    "                name=f\"dropuout_layer_{i}\"\n",
    "            ))\n",
    "\n",
    "        model.add(Dense(self.n_steps, name=\"dense_layer_output\"))\n",
    "        \n",
    "        optimizer = Adam if hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\"]) == \"adam\" else RMSprop\n",
    "        learning_rate = hp.Float(\"lr\", min_value=1e-5, max_value=1e-2)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer(learning_rate=learning_rate), loss=\"mse\"\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuner(\n",
    "    project_name, iterasi, n_features, n_steps=5, max_trials=30, \n",
    "    directory='model', objective='val_rmse', n_splits=10, direction='min', \n",
    "    target_column='close', verbose_fit_model=2\n",
    "):\n",
    "    hypermodel = OneVectorHyperModel(\n",
    "        n_features=n_features,\n",
    "        n_steps=n_steps\n",
    "    )\n",
    "\n",
    "    oracle = kt.oracles.BayesianOptimization(\n",
    "        objective=kt.Objective(objective, direction), \n",
    "        max_trials=max_trials\n",
    "    )\n",
    "\n",
    "    return WFVTuner(\n",
    "        target_column=target_column,\n",
    "        n_splits=n_splits,\n",
    "        n_steps=n_steps,\n",
    "        oracle=oracle, \n",
    "        hypermodel=hypermodel,\n",
    "        directory=directory,\n",
    "        project_name=os.path.join(project_name, f'iterasi_{iterasi}'),\n",
    "        verbose_fit_model=verbose_fit_model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bri = pd.read_csv('data/bri_data.csv').set_index('date', drop=True)\n",
    "\n",
    "data = bri[['close']]\n",
    "tuner_close = get_tuner(\n",
    "    project_name='close', iterasi=4, n_splits=10,\n",
    "    n_features=data.shape[1], max_trials=50, verbose_fit_model=0,\n",
    ")\n",
    "\n",
    "tuner_close.search(data, y=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(model, data):\n",
    "    prediction = model.predict(data.scaled_x)\n",
    "    prediction = data.scaler_y.inverse_transform(prediction.reshape(-1, 1))\n",
    "    return prediction, data.y\n",
    "\n",
    "model, metrics, trial, summary, (data_train, data_es, data_test) = tuner_close.get_file_in_directory()\n",
    "print(model.summary())\n",
    "\n",
    "prediction1, y1 = get_prediction(model, data_es)\n",
    "prediction2, y2 = get_prediction(model, data_test)\n",
    "print(prediction1.reshape(-1), y1.reshape(-1))\n",
    "print(prediction2.reshape(-1), y2.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_size(size_bytes):\n",
    "   if size_bytes == 0:\n",
    "       return \"0B\"\n",
    "   size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "   i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "   p = math.pow(1024, i)\n",
    "   s = round(size_bytes / p, 2)\n",
    "   return \"%s %s\" % (s, size_name[i])\n",
    "\n",
    "\n",
    "def get_size(start_path = '.'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return convert_size(total_size)\n",
    "\n",
    "def zipfolder(foldername, target_dir):            \n",
    "    zipobj = zipfile.ZipFile(foldername + '.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "    rootlen = len(target_dir) + 1\n",
    "    for base, dirs, files in os.walk(target_dir):\n",
    "        for file in files:\n",
    "            fn = os.path.join(base, file)\n",
    "            zipobj.write(fn, fn[rootlen:])\n",
    "\n",
    "print(get_size())\n",
    "zipfolder('model', 'model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('skripsi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e6f7feddf18c8ecc9fa0d09c723cea7fa4b6247af6f300fc40fbf3468d2ad1be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
